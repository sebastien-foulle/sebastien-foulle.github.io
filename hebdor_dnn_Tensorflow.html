<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>  Reseaux de neurones profonds avec TensorFlow  </title>

<script src="site_libs/header-attrs-2.8/header-attrs.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

/* A workaround for https://github.com/jgm/pandoc/issues/4278 */
a.sourceLine {
  pointer-events: auto;
}

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<link rel="stylesheet" href="hebdor_dnn_Tensorflow_files/style.css" type="text/css" />





</head>

<body>




<section class="page-header">
<h1 class="title toc-ignore project-name"> <svg aria-hidden="true" role="img" viewBox="0 0 448 512" style="height:60px;width:52.5px;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:gold;overflow:visible;position:relative;"><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z"/></svg> Reseaux de neurones profonds avec TensorFlow <br></h1>
</section>


<div id="TOC" class="toc">
<div class="toc-box">
<h2 id="toc-title" class="toc-title">Table des matieres</h2>
<ul>
<li><a href="#intro">Introduction</a>
<ul>
<li><a href="#donnees-et-modules">Donnees et modules</a></li>
<li><a href="#pretraitement-des-colonnes">Pretraitement des colonnes</a></li>
<li><a href="#fonction-de-lecture-des-donnees">Fonction de lecture des donnees</a></li>
</ul></li>
<li><a href="#modelisation-sequentielle-avec-keras">Modelisation Sequentielle avec Keras</a>
<ul>
<li><a href="#train-test-validation">Train, test, validation</a></li>
<li><a href="#parametres-et-metriques">Parametres et metriques</a>
<ul>
<li><a href="#architecture-du-modele">Architecture du modele</a></li>
<li><a href="#autres-parametres">Autres parametres</a></li>
<li><a href="#dependances-entre-hyperparametres">Dependances entre hyperparametres</a></li>
</ul></li>
<li><a href="#entrainement-des-modeles">Entrainement des modeles</a></li>
<li><a href="#performance-des-modeles">Performance des modeles</a></li>
</ul></li>
<li><a href="#modelisation-multi-classes-avec-tf.estimator">Modelisation multi-classes avec <em>tf.estimator</em></a></li>
</ul>
</div>
</div>

<section class="main-content">
<style>
.r{background-color:lightgrey !important;}
</style>
<div id="intro" class="section level1">
<h1>Introduction</h1>
<p>On va modeliser ici une cible multi-classe a l’aide de l’interface Keras qui a ete integree a TensorFlow. Elle permet une modelisation rapide et fournit des sorties graphiques sur la qualite du modele.</p>
<p>On reprend le jeu de donnees et les pretraitements du document precedent sur TensorFlow.</p>
<div id="donnees-et-modules" class="section level3">
<h3>Donnees et modules</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;reticulate&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># on a installe tensorflow dans l&#39;environnement vituel conda &quot;tf_env&quot;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">use_condaenv</span>(<span class="at">condaenv =</span> <span class="st">&quot;tf_env&quot;</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="at">package =</span> <span class="st">&quot;arules&quot;</span>, IncomeESL)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>dtf_class <span class="ot">=</span> tidyr<span class="sc">::</span><span class="fu">drop_na</span>(IncomeESL)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(dtf_class) <span class="ot">=</span> <span class="fu">gsub</span>(<span class="st">&quot; &quot;</span>, <span class="st">&quot;_&quot;</span>, <span class="fu">colnames</span>(dtf_class))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>dtf_class <span class="ot">=</span> dtf_class[<span class="fu">c</span>(<span class="st">&quot;number_in_household&quot;</span>, <span class="st">&quot;marital_status&quot;</span>, <span class="st">&quot;householder_status&quot;</span>, <span class="st">&quot;income&quot;</span>)]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>dtf_class<span class="sc">$</span>number_in_household <span class="ot">=</span> <span class="fu">as.character</span>(dtf_class<span class="sc">$</span>number_in_household)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>dtf_class<span class="sc">$</span>number_in_household[dtf_class<span class="sc">$</span>number_in_household <span class="sc">==</span> <span class="st">&quot;9+&quot;</span>]<span class="ot">=</span> <span class="st">&quot;9&quot;</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>dtf_class<span class="sc">$</span>number_in_household <span class="ot">=</span> <span class="fu">as.integer</span>(dtf_class<span class="sc">$</span>number_in_household)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (col_quali <span class="cf">in</span> <span class="fu">colnames</span>(dtf_class)[<span class="fu">sapply</span>(dtf_class, is.factor)]) {</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  dtf_class[[col_quali]] <span class="ot">=</span> <span class="fu">as.character</span>(dtf_class[[col_quali]] )</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>dtf_class<span class="sc">$</span>income <span class="ot">=</span> <span class="fu">gsub</span>(<span class="st">&quot;,&quot;</span>, <span class="st">&quot;-&quot;</span>, dtf_class<span class="sc">$</span>income)</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">&#39;display.max_columns&#39;</span>, <span class="va">None</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shutil</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pprint</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>pp <span class="op">=</span> pprint.PrettyPrinter(indent<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> feature_column</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># pour regler un bug graphique assez courant avec Anaconda, </span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># adapter le chemin vers le dossier &#39;plugins/platforms&#39;</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> os <span class="im">import</span> path</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">&#39;QT_QPA_PLATFORM_PLUGIN_PATH&#39;</span>] <span class="op">=</span> <span class="st">&#39;C:/Users/Sebastien/Anaconda3/Library/plugins/platforms&#39;</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>os.chdir(<span class="st">&quot;.&quot;</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>()</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">2021</span>)</span></code></pre></div>
<p>Les versions Python et TensorFlow utilisees.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>sys.version</span></code></pre></div>
<pre><code>&#39;3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]&#39;</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>tf.__version__</span></code></pre></div>
<pre><code>&#39;2.3.0&#39;</code></pre>
<p>Les donnees sous Python.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>dtf_class <span class="op">=</span> r.dtf_class</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>dtf_class.head()</span></code></pre></div>
<pre><code>   number_in_household marital_status        householder_status   income
0                    5        married                       own      75+
1                    3        married                      rent      75+
2                    4         single  live with parents/family   [0-10)
3                    4         single  live with parents/family   [0-10)
4                    2        married                       own  [50-75)</code></pre>
<p>Frequences des modalites de la cible.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>dtf_class.income.value_counts(normalize<span class="op">=</span><span class="va">True</span>).sort_index()</span></code></pre></div>
<pre><code>75+        0.108057
[0-10)     0.182519
[10-15)    0.076934
[15-20)    0.073444
[20-25)    0.089878
[25-30)    0.076643
[30-40)    0.123037
[40-50)    0.114020
[50-75)    0.155468
Name: income, dtype: float64</code></pre>
<p>Dictionnaire pour recoder la variable cible.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># mapping pour recoder les modalites cibles en entiers</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>dico_income <span class="op">=</span> {<span class="st">&#39;[0-10)&#39;</span>:<span class="dv">0</span>,<span class="st">&#39;[10-15)&#39;</span>:<span class="dv">1</span>,<span class="st">&#39;[15-20)&#39;</span>:<span class="dv">2</span>, <span class="st">&#39;[20-25)&#39;</span>:<span class="dv">3</span>,<span class="st">&#39;[25-30)&#39;</span>:<span class="dv">4</span>, <span class="st">&#39;[30-40)&#39;</span>:<span class="dv">5</span>, <span class="st">&#39;[40-50)&#39;</span>:<span class="dv">6</span>, <span class="st">&#39;[50-75)&#39;</span>:<span class="dv">7</span>,<span class="st">&#39;75+&#39;</span>:<span class="dv">8</span>}</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>dico_income_bin <span class="op">=</span> {<span class="st">&#39;[0-10)&#39;</span>:<span class="dv">0</span>,<span class="st">&#39;[10-15)&#39;</span>:<span class="dv">0</span>,<span class="st">&#39;[15-20)&#39;</span>:<span class="dv">0</span>, <span class="st">&#39;[20-25)&#39;</span>:<span class="dv">0</span>,<span class="st">&#39;[25-30)&#39;</span>:<span class="dv">0</span>, <span class="st">&#39;[30-40)&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;[40-50)&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;[50-75)&#39;</span>:<span class="dv">1</span>,<span class="st">&#39;75+&#39;</span>:<span class="dv">1</span>}</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>pp.pprint(dico_income)</span></code></pre></div>
<pre><code>{   &#39;75+&#39;: 8,
    &#39;[0-10)&#39;: 0,
    &#39;[10-15)&#39;: 1,
    &#39;[15-20)&#39;: 2,
    &#39;[20-25)&#39;: 3,
    &#39;[25-30)&#39;: 4,
    &#39;[30-40)&#39;: 5,
    &#39;[40-50)&#39;: 6,
    &#39;[50-75)&#39;: 7}</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>pp.pprint(dico_income_bin)</span></code></pre></div>
<pre><code>{   &#39;75+&#39;: 1,
    &#39;[0-10)&#39;: 0,
    &#39;[10-15)&#39;: 0,
    &#39;[15-20)&#39;: 0,
    &#39;[20-25)&#39;: 0,
    &#39;[25-30)&#39;: 0,
    &#39;[30-40)&#39;: 1,
    &#39;[40-50)&#39;: 1,
    &#39;[50-75)&#39;: 1}</code></pre>
</div>
<div id="pretraitement-des-colonnes" class="section level3">
<h3>Pretraitement des colonnes</h3>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>feature_columns <span class="op">=</span> []</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># numeriques</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>feature_columns.append(feature_column.numeric_column(<span class="st">&quot;number_in_household&quot;</span>))</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># en buckets</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>number_in_household <span class="op">=</span> feature_column.numeric_column(<span class="st">&quot;number_in_household&quot;</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>number_bucket <span class="op">=</span> tf.feature_column.bucketized_column(number_in_household, boundaries <span class="op">=</span> [<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>])</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>feature_columns.append(number_bucket)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># indicatrices</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>marital_status <span class="op">=</span> tf.feature_column.categorical_column_with_vocabulary_list(</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>  <span class="st">&#39;marital_status&#39;</span>, [<span class="st">&#39;single&#39;</span>, <span class="st">&#39;married&#39;</span>, <span class="st">&#39;divorced&#39;</span>, <span class="st">&#39;cohabitation&#39;&#39;, widowed&#39;</span>])</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>marital_one_hot <span class="op">=</span> tf.feature_column.indicator_column(marital_status)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>feature_columns.append(marital_one_hot)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co"># embedding</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>householder_status <span class="op">=</span> tf.feature_column.categorical_column_with_vocabulary_list(</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>  <span class="st">&#39;householder_status&#39;</span>, [<span class="st">&#39;own&#39;</span>, <span class="st">&#39;rent&#39;</span>, <span class="st">&#39;live with parents/family&#39;</span>])</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>householder_embedding <span class="op">=</span> tf.feature_column.embedding_column(householder_status, dimension <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>feature_columns.append(householder_embedding)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a><span class="co"># interactions</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>interactions <span class="op">=</span> tf.feature_column.crossed_column([marital_status, number_bucket], </span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>hash_bucket_size <span class="op">=</span> <span class="dv">7</span>, hash_key <span class="op">=</span> <span class="dv">8</span>)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>interactions <span class="op">=</span> tf.feature_column.indicator_column(interactions)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>feature_columns.append(interactions)</span></code></pre></div>
<p>Dictionnaire pour la lisibilite des operations effectuees, une liste suffirait techniquement.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>feature_columns <span class="op">=</span> {<span class="st">&quot;number_in_household&quot;</span>:feature_column.numeric_column(<span class="st">&quot;number_in_household&quot;</span>),</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;number_bucket&quot;</span>: number_bucket,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;marital_one_hot&quot;</span>: marital_one_hot,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;householder_embedding&quot;</span>: householder_embedding,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;interactions&quot;</span>: interactions</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
</div>
<div id="fonction-de-lecture-des-donnees" class="section level3">
<h3>Fonction de lecture des donnees</h3>
<p>Par defaut on considere que la cible est multi-classes et on recode ses modalites de 0 a 8. Si on prend <em>dico = dico_income_bin</em> on considere que la cible est binaire et recodee en 0/1.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># on cree un tf.data.dataset a patir du Dataframe</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> df_to_dataset(dataframe, shuffle <span class="op">=</span> <span class="va">True</span>, batch_size <span class="op">=</span> <span class="dv">32</span>, dico <span class="op">=</span> dico_income, nb_repet <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  dataframe <span class="op">=</span> dataframe.copy()</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  labels <span class="op">=</span> dataframe.pop(<span class="st">&#39;income&#39;</span>).<span class="bu">map</span>(dico)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  ds <span class="op">=</span> tf.data.Dataset.from_tensor_slices((<span class="bu">dict</span>(dataframe), labels))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> shuffle:</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># permutation dde toutes les lignes du dataframe car seulement quelques milliers de lignes</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    ds <span class="op">=</span> ds.shuffle(buffer_size <span class="op">=</span> <span class="bu">len</span>(dataframe))</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>  ds <span class="op">=</span> ds.repeat(count <span class="op">=</span> nb_repet).batch(batch_size)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> ds</span></code></pre></div>
</div>
</div>
<div id="modelisation-sequentielle-avec-keras" class="section level1">
<h1>Modelisation Sequentielle avec Keras</h1>
<div id="train-test-validation" class="section level3">
<h3>Train, test, validation</h3>
<p>Les trois jeux de donnees pour l’apprentissage, les validations et le test final.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># test = 20% du total</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>train, test <span class="op">=</span> train_test_split(dtf_class, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state <span class="op">=</span> <span class="dv">2021</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># val = 25% du train = 20% du total</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>train, val <span class="op">=</span> train_test_split(train, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state <span class="op">=</span> <span class="dv">2021</span>)</span></code></pre></div>
</div>
<div id="parametres-et-metriques" class="section level3">
<h3>Parametres et metriques</h3>
<div id="architecture-du-modele" class="section level4">
<h4>Architecture du modele</h4>
<p>Un reseau de neurones avec Keras Sequentiel est une succession de couches de neurones, ici on a :</p>
<ul>
<li>une couche <em>DenseFeatures</em> de conversion des entrees en tenseurs</li>
<li>deux couches cachees avec respectivement 16 et 8 neurones et la fonction d’activation standard “relu” pour les couches cachees</li>
<li>une couche <em>Dropout</em> qui s"applique a la couche precedente de 8 neurones. Elle permet de lutter contre le surapprentissage en supprimant avec une probabilite de 20% chacun des 8 neurones lors de l’entrainement du reseau. Ca n’a de reel interet que pour des reseaux vraiments profonds, c’est-a-dire avec de nombreuses couches cachees.</li>
<li>une couche de sortie avec un score par modalite de la cible. La fonction <em>softmax</em> generalise la sigmoide qui ne s’applique qu’aux cibles binaires.</li>
</ul>
<p>Le nombre de couches cachees, le nombre de neurones de chaque couche et le taux de dropout sont autant d’hyperparametres a optimiser.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>modele_softmax <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  layers.DenseFeatures(feature_columns <span class="op">=</span> feature_columns.values()),</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  layers.Dense(<span class="dv">16</span>, activation <span class="op">=</span> <span class="st">&#39;relu&#39;</span>, name <span class="op">=</span> <span class="st">&quot;hidden1&quot;</span>),</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  layers.Dense(<span class="dv">8</span>, activation <span class="op">=</span> <span class="st">&#39;relu&#39;</span>, name <span class="op">=</span> <span class="st">&quot;hidden2&quot;</span>),</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  layers.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  layers.Dense(<span class="bu">len</span>(dico_income), activation <span class="op">=</span> <span class="st">&#39;softmax&#39;</span>, name <span class="op">=</span> <span class="st">&quot;output&quot;</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>])</span></code></pre></div>
<p>Une variante avec cible binaire.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>modele_binaire <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  layers.DenseFeatures(feature_columns <span class="op">=</span> feature_columns.values()),</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  layers.Dense(<span class="dv">16</span>, activation <span class="op">=</span> <span class="st">&#39;relu&#39;</span>, name <span class="op">=</span> <span class="st">&quot;hidden1&quot;</span>),</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  layers.Dense(<span class="dv">8</span>, activation <span class="op">=</span> <span class="st">&#39;relu&#39;</span>, name <span class="op">=</span> <span class="st">&quot;hidden2&quot;</span>),</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>  layers.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>  layers.Dense(<span class="dv">1</span>, activation <span class="op">=</span> <span class="st">&#39;sigmoid&#39;</span>, name <span class="op">=</span> <span class="st">&quot;output&quot;</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>])</span></code></pre></div>
</div>
<div id="autres-parametres" class="section level4">
<h4>Autres parametres</h4>
<p>On precise la valeur de certains parametres dans la fonction <em>compile</em> plus bas :</p>
<ul>
<li>hyperparametre <em>optimizer</em> : le choix de l’algorithme de descente de gradient, ils n’ont pas tous les memes parametres. Ainsi l’optimiseur <em>Ftrl</em> propose des ratios de regularisation, voir <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl" class="uri">https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl</a></li>
<li>hyperparametre <em>loss</em> : la fonction de perte a minimiser par l’algorithme de descente de gradient, la “cross-entropy” n’est rien d’autre que l’oppose de la log-vraisemblance
<ul>
<li><em>tf.keras.losses.BinaryCrossentropy</em> (ou ‘binary_crossentropy’) en cas de cible binaire 0/1</li>
<li><em>tf.keras.losses.CategoricalCrossentropy</em> (ou ‘categorical_crossentropy’) si cible multi-classes au format “une indicatrice par classe”</li>
<li><em>tf.keras.losses.SparseCategoricalCrossentropy</em> (ou ‘sparse_categorical_crossentropy’) si cible multi-classes au format “colonne d’entiers”</li>
</ul></li>
<li><em>metrics</em> : les metriques metiers classiques pour evaluer la performance du modele. Les metriques ne sont pas des hyperparametres du modele : elles n’interviennent pas dans l’entrainement du modele. Si la cible est binaire on a entre autres
<ul>
<li>l’AUC</li>
<li>l’exactitude toujours fournie par defaut</li>
<li>la precision</li>
<li>le rappel</li>
<li>les metriques de comptages des vrais positifs, faux negatifs, … de la matrice de confusion</li>
</ul></li>
</ul>
<p>Des metriques pour le cas d’une cible binaire.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>metriques_bin <span class="op">=</span> [tf.keras.metrics.AUC(name<span class="op">=</span><span class="st">&#39;auc&#39;</span>),</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  tf.keras.metrics.BinaryAccuracy(name<span class="op">=</span><span class="st">&#39;accuracy&#39;</span>),</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>  tf.keras.metrics.Precision(name<span class="op">=</span><span class="st">&#39;precision&#39;</span>),</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>  tf.keras.metrics.Recall(name<span class="op">=</span><span class="st">&#39;recall&#39;</span>),</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>  tf.keras.metrics.TruePositives(name<span class="op">=</span><span class="st">&#39;tp&#39;</span>),</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>  tf.keras.metrics.FalsePositives(name<span class="op">=</span><span class="st">&#39;fp&#39;</span>),</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>  tf.keras.metrics.TrueNegatives(name<span class="op">=</span><span class="st">&#39;tn&#39;</span>),</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>  tf.keras.metrics.FalseNegatives(name<span class="op">=</span><span class="st">&#39;fn&#39;</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>  ]</span></code></pre></div>
</div>
<div id="dependances-entre-hyperparametres" class="section level4">
<h4>Dependances entre hyperparametres</h4>
<p>On suit les recommandations de l’article <a href="https://medium.com/google-cloud/ml-design-pattern-3-virtual-epochs-f842296de730" class="uri">https://medium.com/google-cloud/ml-design-pattern-3-virtual-epochs-f842296de730</a> qui deconseille de choisir directement le nombre d’epoques ou d’etapes (c’est-a-dire de mini-batchs) mais plutot</p>
<ul>
<li>la taille des mini-batchs : <em>TRAIN_BATCH_SIZE</em></li>
<li>le nombre total d’exemples montres au modele pendant son entrainement : <em>NUM_TRAIN_EXAMPLES</em></li>
<li>le nombre de checkpoints, a chaque checkpoint on stocke les performances du modele qu’on va representer graphiquement : <em>NUM_CHECKPOINTS</em></li>
</ul>
<p>Ici les “epoques” sont virtuelles et ne correspondent pas tout a fait a la lecture de tout le dataframe, comme l’indique la documentation <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit" class="uri">https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit</a></p>
<ul>
<li>si l’argument <em>steps_per_epoch</em> est renseigne, une epoque virtuelle est consideree achevee quand on a lu <em>steps_per_epoch</em> * <em>TRAIN_BATCH_SIZE</em> exemples</li>
<li>a la fin de chaque epoque virtuelle des statistiques sont calculees, voir <a href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/BaseLogger" class="uri">https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/BaseLogger</a></li>
</ul>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>tf.random.set_seed(<span class="dv">2021</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>TRAIN_BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>NUM_TRAIN_EXAMPLES <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>NUM_CHECKPOINTS <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>steps_per_epoch <span class="op">=</span> NUM_TRAIN_EXAMPLES <span class="op">//</span> (TRAIN_BATCH_SIZE <span class="op">*</span> NUM_CHECKPOINTS)</span></code></pre></div>
</div>
</div>
<div id="entrainement-des-modeles" class="section level3">
<h3>Entrainement des modeles</h3>
<p>On entraine d’abord le modele multi-classes. Les jeu de donnees de validation et de test n’ont pas besoin d’etre parcourus plusieurs fois ou d’etre permutes, contrairement au jeu d’apprentissage.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> df_to_dataset(train, batch_size <span class="op">=</span> TRAIN_BATCH_SIZE)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>val_ds <span class="op">=</span> df_to_dataset(val, shuffle <span class="op">=</span> <span class="va">False</span>, batch_size <span class="op">=</span> TRAIN_BATCH_SIZE, nb_repet <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>test_ds <span class="op">=</span> df_to_dataset(test, shuffle <span class="op">=</span> <span class="va">False</span>, batch_size <span class="op">=</span> TRAIN_BATCH_SIZE, nb_repet <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co"># on configure le modele</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>modele_softmax.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">&#39;adam&#39;</span>,</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>              loss <span class="op">=</span> tf.keras.losses.SparseCategoricalCrossentropy(),</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>              metrics <span class="op">=</span> [<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>history_sofmax <span class="op">=</span> modele_softmax.fit(x <span class="op">=</span> train_ds,</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>                    steps_per_epoch <span class="op">=</span> steps_per_epoch,</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>                    epochs <span class="op">=</span> NUM_CHECKPOINTS,</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>                    validation_data <span class="op">=</span> val_ds</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>                   )</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>                   </span></code></pre></div>
<pre><code>Epoch 1/20

  1/156 [..............................] - ETA: 0s - loss: 2.2057 - accuracy: 0.1875
 18/156 [==&gt;...........................] - ETA: 0s - loss: 2.2139 - accuracy: 0.1224
 35/156 [=====&gt;........................] - ETA: 0s - loss: 2.1987 - accuracy: 0.1205
 48/156 [========&gt;.....................] - ETA: 0s - loss: 2.1897 - accuracy: 0.1279
 62/156 [==========&gt;...................] - ETA: 0s - loss: 2.1817 - accuracy: 0.1376
 76/156 [=============&gt;................] - ETA: 0s - loss: 2.1739 - accuracy: 0.1482
 96/156 [=================&gt;............] - ETA: 0s - loss: 2.1665 - accuracy: 0.1618
113/156 [====================&gt;.........] - ETA: 0s - loss: 2.1609 - accuracy: 0.1702
134/156 [========================&gt;.....] - ETA: 0s - loss: 2.1551 - accuracy: 0.1743
156/156 [==============================] - 2s 10ms/step - loss: 2.1483 - accuracy: 0.1792 - val_loss: 2.0989 - val_accuracy: 0.2284
Epoch 2/20

  1/156 [..............................] - ETA: 0s - loss: 2.1007 - accuracy: 0.2031
 21/156 [===&gt;..........................] - ETA: 0s - loss: 2.0925 - accuracy: 0.2106
 42/156 [=======&gt;......................] - ETA: 0s - loss: 2.0775 - accuracy: 0.2251
 63/156 [===========&gt;..................] - ETA: 0s - loss: 2.0684 - accuracy: 0.2297
 83/156 [==============&gt;...............] - ETA: 0s - loss: 2.0648 - accuracy: 0.2263
106/156 [===================&gt;..........] - ETA: 0s - loss: 2.0567 - accuracy: 0.2311
127/156 [=======================&gt;......] - ETA: 0s - loss: 2.0498 - accuracy: 0.2345
146/156 [===========================&gt;..] - ETA: 0s - loss: 2.0480 - accuracy: 0.2342
156/156 [==============================] - 0s 3ms/step - loss: 2.0454 - accuracy: 0.2353 - val_loss: 2.0053 - val_accuracy: 0.2458
Epoch 3/20

  1/156 [..............................] - ETA: 0s - loss: 2.0419 - accuracy: 0.2656
 21/156 [===&gt;..........................] - ETA: 0s - loss: 2.0125 - accuracy: 0.2478
 41/156 [======&gt;.......................] - ETA: 0s - loss: 2.0124 - accuracy: 0.2519
 63/156 [===========&gt;..................] - ETA: 0s - loss: 2.0021 - accuracy: 0.2589
 83/156 [==============&gt;...............] - ETA: 0s - loss: 1.9955 - accuracy: 0.2605
105/156 [===================&gt;..........] - ETA: 0s - loss: 1.9871 - accuracy: 0.2641
125/156 [=======================&gt;......] - ETA: 0s - loss: 1.9883 - accuracy: 0.2609
147/156 [===========================&gt;..] - ETA: 0s - loss: 1.9890 - accuracy: 0.2607
156/156 [==============================] - 0s 3ms/step - loss: 1.9865 - accuracy: 0.2613 - val_loss: 1.9649 - val_accuracy: 0.2705
Epoch 4/20

  1/156 [..............................] - ETA: 0s - loss: 2.0115 - accuracy: 0.2656
 21/156 [===&gt;..........................] - ETA: 0s - loss: 1.9612 - accuracy: 0.2872
 41/156 [======&gt;.......................] - ETA: 0s - loss: 1.9631 - accuracy: 0.2752
 62/156 [==========&gt;...................] - ETA: 0s - loss: 1.9636 - accuracy: 0.2692
 83/156 [==============&gt;...............] - ETA: 0s - loss: 1.9590 - accuracy: 0.2722
107/156 [===================&gt;..........] - ETA: 0s - loss: 1.9590 - accuracy: 0.2709
129/156 [=======================&gt;......] - ETA: 0s - loss: 1.9591 - accuracy: 0.2706
150/156 [===========================&gt;..] - ETA: 0s - loss: 1.9553 - accuracy: 0.2733
156/156 [==============================] - 0s 3ms/step - loss: 1.9555 - accuracy: 0.2741 - val_loss: 1.9470 - val_accuracy: 0.2698
Epoch 5/20

  1/156 [..............................] - ETA: 0s - loss: 2.0049 - accuracy: 0.1875
 22/156 [===&gt;..........................] - ETA: 0s - loss: 1.9476 - accuracy: 0.2770
 43/156 [=======&gt;......................] - ETA: 0s - loss: 1.9455 - accuracy: 0.2794
 66/156 [===========&gt;..................] - ETA: 0s - loss: 1.9491 - accuracy: 0.2789
 91/156 [================&gt;.............] - ETA: 0s - loss: 1.9417 - accuracy: 0.2831
115/156 [=====================&gt;........] - ETA: 0s - loss: 1.9444 - accuracy: 0.2799
137/156 [=========================&gt;....] - ETA: 0s - loss: 1.9417 - accuracy: 0.2809
156/156 [==============================] - 0s 3ms/step - loss: 1.9430 - accuracy: 0.2799 - val_loss: 1.9373 - val_accuracy: 0.2676
Epoch 6/20

  1/156 [..............................] - ETA: 0s - loss: 1.8487 - accuracy: 0.2812
 22/156 [===&gt;..........................] - ETA: 0s - loss: 1.9353 - accuracy: 0.2763
 43/156 [=======&gt;......................] - ETA: 0s - loss: 1.9265 - accuracy: 0.2867
 67/156 [===========&gt;..................] - ETA: 0s - loss: 1.9349 - accuracy: 0.2843
 87/156 [===============&gt;..............] - ETA: 0s - loss: 1.9371 - accuracy: 0.2832
107/156 [===================&gt;..........] - ETA: 0s - loss: 1.9378 - accuracy: 0.2827
135/156 [========================&gt;.....] - ETA: 0s - loss: 1.9359 - accuracy: 0.2837
156/156 [==============================] - ETA: 0s - loss: 1.9366 - accuracy: 0.2837
156/156 [==============================] - 0s 3ms/step - loss: 1.9366 - accuracy: 0.2837 - val_loss: 1.9336 - val_accuracy: 0.2676
Epoch 7/20

  1/156 [..............................] - ETA: 0s - loss: 1.8853 - accuracy: 0.3125
 22/156 [===&gt;..........................] - ETA: 0s - loss: 1.9240 - accuracy: 0.2848
 47/156 [========&gt;.....................] - ETA: 0s - loss: 1.9367 - accuracy: 0.2842
 69/156 [============&gt;.................] - ETA: 0s - loss: 1.9340 - accuracy: 0.2889
 90/156 [================&gt;.............] - ETA: 0s - loss: 1.9282 - accuracy: 0.2889
109/156 [===================&gt;..........] - ETA: 0s - loss: 1.9288 - accuracy: 0.2878
125/156 [=======================&gt;......] - ETA: 0s - loss: 1.9354 - accuracy: 0.2849
140/156 [=========================&gt;....] - ETA: 0s - loss: 1.9324 - accuracy: 0.2858
156/156 [==============================] - 1s 3ms/step - loss: 1.9304 - accuracy: 0.2874 - val_loss: 1.9281 - val_accuracy: 0.2705
Epoch 8/20

  1/156 [..............................] - ETA: 0s - loss: 1.8391 - accuracy: 0.3594
 16/156 [==&gt;...........................] - ETA: 0s - loss: 1.8936 - accuracy: 0.2900
 32/156 [=====&gt;........................] - ETA: 0s - loss: 1.9178 - accuracy: 0.2949
 48/156 [========&gt;.....................] - ETA: 0s - loss: 1.9196 - accuracy: 0.2910
 64/156 [===========&gt;..................] - ETA: 0s - loss: 1.9286 - accuracy: 0.2844
 84/156 [===============&gt;..............] - ETA: 0s - loss: 1.9218 - accuracy: 0.2919
100/156 [==================&gt;...........] - ETA: 0s - loss: 1.9208 - accuracy: 0.2920
121/156 [======================&gt;.......] - ETA: 0s - loss: 1.9198 - accuracy: 0.2912
142/156 [==========================&gt;...] - ETA: 0s - loss: 1.9234 - accuracy: 0.2903
156/156 [==============================] - 1s 3ms/step - loss: 1.9253 - accuracy: 0.2896 - val_loss: 1.9262 - val_accuracy: 0.2749
Epoch 9/20

  1/156 [..............................] - ETA: 0s - loss: 2.0118 - accuracy: 0.2969
 22/156 [===&gt;..........................] - ETA: 0s - loss: 1.9282 - accuracy: 0.2848
 42/156 [=======&gt;......................] - ETA: 0s - loss: 1.9234 - accuracy: 0.2820
 62/156 [==========&gt;...................] - ETA: 0s - loss: 1.9234 - accuracy: 0.2845
 86/156 [===============&gt;..............] - ETA: 0s - loss: 1.9225 - accuracy: 0.2885
106/156 [===================&gt;..........] - ETA: 0s - loss: 1.9231 - accuracy: 0.2910
127/156 [=======================&gt;......] - ETA: 0s - loss: 1.9256 - accuracy: 0.2899
151/156 [============================&gt;.] - ETA: 0s - loss: 1.9263 - accuracy: 0.2918
156/156 [==============================] - 0s 3ms/step - loss: 1.9250 - accuracy: 0.2919 - val_loss: 1.9256 - val_accuracy: 0.2749
Epoch 10/20

  1/156 [..............................] - ETA: 0s - loss: 1.8372 - accuracy: 0.3906
 23/156 [===&gt;..........................] - ETA: 0s - loss: 1.9384 - accuracy: 0.2914
 44/156 [=======&gt;......................] - ETA: 0s - loss: 1.9277 - accuracy: 0.2887
 67/156 [===========&gt;..................] - ETA: 0s - loss: 1.9138 - accuracy: 0.2941
 91/156 [================&gt;.............] - ETA: 0s - loss: 1.9162 - accuracy: 0.2936
112/156 [====================&gt;.........] - ETA: 0s - loss: 1.9196 - accuracy: 0.2931
136/156 [=========================&gt;....] - ETA: 0s - loss: 1.9219 - accuracy: 0.2909
156/156 [==============================] - 0s 3ms/step - loss: 1.9178 - accuracy: 0.2942 - val_loss: 1.9242 - val_accuracy: 0.2742
Epoch 11/20

  1/156 [..............................] - ETA: 0s - loss: 1.9566 - accuracy: 0.3438
 19/156 [==&gt;...........................] - ETA: 0s - loss: 1.9308 - accuracy: 0.3002
 46/156 [=======&gt;......................] - ETA: 0s - loss: 1.9294 - accuracy: 0.2867
 66/156 [===========&gt;..................] - ETA: 0s - loss: 1.9276 - accuracy: 0.2888
 87/156 [===============&gt;..............] - ETA: 0s - loss: 1.9258 - accuracy: 0.2913
109/156 [===================&gt;..........] - ETA: 0s - loss: 1.9254 - accuracy: 0.2926
132/156 [========================&gt;.....] - ETA: 0s - loss: 1.9244 - accuracy: 0.2926
156/156 [==============================] - ETA: 0s - loss: 1.9231 - accuracy: 0.2912
156/156 [==============================] - 0s 3ms/step - loss: 1.9231 - accuracy: 0.2912 - val_loss: 1.9219 - val_accuracy: 0.2742
Epoch 12/20

  1/156 [..............................] - ETA: 0s - loss: 1.9455 - accuracy: 0.2188
 30/156 [====&gt;.........................] - ETA: 0s - loss: 1.9343 - accuracy: 0.2953
 54/156 [=========&gt;....................] - ETA: 0s - loss: 1.9241 - accuracy: 0.2859
 79/156 [==============&gt;...............] - ETA: 0s - loss: 1.9211 - accuracy: 0.2953
103/156 [==================&gt;...........] - ETA: 0s - loss: 1.9185 - accuracy: 0.2941
128/156 [=======================&gt;......] - ETA: 0s - loss: 1.9190 - accuracy: 0.2944
152/156 [============================&gt;.] - ETA: 0s - loss: 1.9209 - accuracy: 0.2941
156/156 [==============================] - 0s 2ms/step - loss: 1.9220 - accuracy: 0.2936 - val_loss: 1.9223 - val_accuracy: 0.2749
Epoch 13/20

  1/156 [..............................] - ETA: 0s - loss: 2.0110 - accuracy: 0.2656
 26/156 [====&gt;.........................] - ETA: 0s - loss: 1.9215 - accuracy: 0.3017
 51/156 [========&gt;.....................] - ETA: 0s - loss: 1.9200 - accuracy: 0.2950
 74/156 [=============&gt;................] - ETA: 0s - loss: 1.8984 - accuracy: 0.3057
 98/156 [=================&gt;............] - ETA: 0s - loss: 1.9057 - accuracy: 0.3042
127/156 [=======================&gt;......] - ETA: 0s - loss: 1.9126 - accuracy: 0.3017
151/156 [============================&gt;.] - ETA: 0s - loss: 1.9105 - accuracy: 0.3014
156/156 [==============================] - 0s 2ms/step - loss: 1.9104 - accuracy: 0.2996 - val_loss: 1.9215 - val_accuracy: 0.2742
Epoch 14/20

  1/156 [..............................] - ETA: 0s - loss: 1.9884 - accuracy: 0.2656
 29/156 [====&gt;.........................] - ETA: 0s - loss: 1.9157 - accuracy: 0.3093
 53/156 [=========&gt;....................] - ETA: 0s - loss: 1.9163 - accuracy: 0.3034
 77/156 [=============&gt;................] - ETA: 0s - loss: 1.9205 - accuracy: 0.2975
106/156 [===================&gt;..........] - ETA: 0s - loss: 1.9189 - accuracy: 0.3009
135/156 [========================&gt;.....] - ETA: 0s - loss: 1.9184 - accuracy: 0.3007
156/156 [==============================] - 0s 2ms/step - loss: 1.9144 - accuracy: 0.2998 - val_loss: 1.9208 - val_accuracy: 0.2742
Epoch 15/20

  1/156 [..............................] - ETA: 0s - loss: 2.0066 - accuracy: 0.2656
 27/156 [====&gt;.........................] - ETA: 0s - loss: 1.9302 - accuracy: 0.2801
 53/156 [=========&gt;....................] - ETA: 0s - loss: 1.9134 - accuracy: 0.2916
 77/156 [=============&gt;................] - ETA: 0s - loss: 1.9135 - accuracy: 0.2926
104/156 [===================&gt;..........] - ETA: 0s - loss: 1.9079 - accuracy: 0.2945
130/156 [========================&gt;.....] - ETA: 0s - loss: 1.9130 - accuracy: 0.2935
155/156 [============================&gt;.] - ETA: 0s - loss: 1.9100 - accuracy: 0.2950
156/156 [==============================] - 0s 2ms/step - loss: 1.9084 - accuracy: 0.2960 - val_loss: 1.9197 - val_accuracy: 0.2764
Epoch 16/20

  1/156 [..............................] - ETA: 0s - loss: 1.9602 - accuracy: 0.2656
 22/156 [===&gt;..........................] - ETA: 0s - loss: 1.9540 - accuracy: 0.2798
 51/156 [========&gt;.....................] - ETA: 0s - loss: 1.9368 - accuracy: 0.2788
 77/156 [=============&gt;................] - ETA: 0s - loss: 1.9212 - accuracy: 0.2873
106/156 [===================&gt;..........] - ETA: 0s - loss: 1.9226 - accuracy: 0.2886
131/156 [========================&gt;.....] - ETA: 0s - loss: 1.9213 - accuracy: 0.2901
155/156 [============================&gt;.] - ETA: 0s - loss: 1.9192 - accuracy: 0.2926
156/156 [==============================] - 0s 2ms/step - loss: 1.9199 - accuracy: 0.2923 - val_loss: 1.9207 - val_accuracy: 0.2764
Epoch 17/20

  1/156 [..............................] - ETA: 0s - loss: 1.8909 - accuracy: 0.3594
 30/156 [====&gt;.........................] - ETA: 0s - loss: 1.9202 - accuracy: 0.2917
 55/156 [=========&gt;....................] - ETA: 0s - loss: 1.9132 - accuracy: 0.2977
 84/156 [===============&gt;..............] - ETA: 0s - loss: 1.9075 - accuracy: 0.3023
112/156 [====================&gt;.........] - ETA: 0s - loss: 1.9120 - accuracy: 0.2998
132/156 [========================&gt;.....] - ETA: 0s - loss: 1.9094 - accuracy: 0.2996
151/156 [============================&gt;.] - ETA: 0s - loss: 1.9103 - accuracy: 0.2984
156/156 [==============================] - 0s 3ms/step - loss: 1.9102 - accuracy: 0.2982 - val_loss: 1.9197 - val_accuracy: 0.2771
Epoch 18/20

  1/156 [..............................] - ETA: 0s - loss: 1.9674 - accuracy: 0.3438
 16/156 [==&gt;...........................] - ETA: 0s - loss: 1.9274 - accuracy: 0.2939
 33/156 [=====&gt;........................] - ETA: 0s - loss: 1.9197 - accuracy: 0.2969
 47/156 [========&gt;.....................] - ETA: 0s - loss: 1.9057 - accuracy: 0.3005
 60/156 [==========&gt;...................] - ETA: 0s - loss: 1.9016 - accuracy: 0.3029
 77/156 [=============&gt;................] - ETA: 0s - loss: 1.9037 - accuracy: 0.2985
101/156 [==================&gt;...........] - ETA: 0s - loss: 1.9048 - accuracy: 0.2995
125/156 [=======================&gt;......] - ETA: 0s - loss: 1.9107 - accuracy: 0.2964
151/156 [============================&gt;.] - ETA: 0s - loss: 1.9093 - accuracy: 0.2960
156/156 [==============================] - 0s 3ms/step - loss: 1.9093 - accuracy: 0.2961 - val_loss: 1.9201 - val_accuracy: 0.2749
Epoch 19/20

  1/156 [..............................] - ETA: 0s - loss: 1.8487 - accuracy: 0.2969
 24/156 [===&gt;..........................] - ETA: 0s - loss: 1.8941 - accuracy: 0.3125
 49/156 [========&gt;.....................] - ETA: 0s - loss: 1.8922 - accuracy: 0.3074
 79/156 [==============&gt;...............] - ETA: 0s - loss: 1.8984 - accuracy: 0.3008
104/156 [===================&gt;..........] - ETA: 0s - loss: 1.9002 - accuracy: 0.3011
129/156 [=======================&gt;......] - ETA: 0s - loss: 1.9006 - accuracy: 0.3009
152/156 [============================&gt;.] - ETA: 0s - loss: 1.9062 - accuracy: 0.2998
156/156 [==============================] - 0s 2ms/step - loss: 1.9064 - accuracy: 0.2994 - val_loss: 1.9185 - val_accuracy: 0.2742
Epoch 20/20

  1/156 [..............................] - ETA: 0s - loss: 1.8796 - accuracy: 0.2500
 23/156 [===&gt;..........................] - ETA: 0s - loss: 1.8840 - accuracy: 0.3050
 46/156 [=======&gt;......................] - ETA: 0s - loss: 1.9018 - accuracy: 0.2965
 75/156 [=============&gt;................] - ETA: 0s - loss: 1.9106 - accuracy: 0.2960
 98/156 [=================&gt;............] - ETA: 0s - loss: 1.9106 - accuracy: 0.2950
123/156 [======================&gt;.......] - ETA: 0s - loss: 1.9065 - accuracy: 0.2970
151/156 [============================&gt;.] - ETA: 0s - loss: 1.9073 - accuracy: 0.2967
156/156 [==============================] - 0s 2ms/step - loss: 1.9085 - accuracy: 0.2956 - val_loss: 1.9181 - val_accuracy: 0.2764

WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a &lt;class &#39;dict&#39;&gt; input: {&#39;number_in_household&#39;: &lt;tf.Tensor &#39;ExpandDims_2:0&#39; shape=(None, 1) dtype=int32&gt;, &#39;marital_status&#39;: &lt;tf.Tensor &#39;ExpandDims_1:0&#39; shape=(None, 1) dtype=string&gt;, &#39;householder_status&#39;: &lt;tf.Tensor &#39;ExpandDims:0&#39; shape=(None, 1) dtype=string&gt;}
Consider rewriting this model with the Functional API.
WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a &lt;class &#39;dict&#39;&gt; input: {&#39;number_in_household&#39;: &lt;tf.Tensor &#39;ExpandDims_2:0&#39; shape=(None, 1) dtype=int32&gt;, &#39;marital_status&#39;: &lt;tf.Tensor &#39;ExpandDims_1:0&#39; shape=(None, 1) dtype=string&gt;, &#39;householder_status&#39;: &lt;tf.Tensor &#39;ExpandDims:0&#39; shape=(None, 1) dtype=string&gt;}
Consider rewriting this model with the Functional API.
WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a &lt;class &#39;dict&#39;&gt; input: {&#39;number_in_household&#39;: &lt;tf.Tensor &#39;ExpandDims_2:0&#39; shape=(None, 1) dtype=int32&gt;, &#39;marital_status&#39;: &lt;tf.Tensor &#39;ExpandDims_1:0&#39; shape=(None, 1) dtype=string&gt;, &#39;householder_status&#39;: &lt;tf.Tensor &#39;ExpandDims:0&#39; shape=(None, 1) dtype=string&gt;}
Consider rewriting this model with the Functional API.</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>modele_softmax.summary()</span></code></pre></div>
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_features (DenseFeature multiple                  6         
_________________________________________________________________
hidden1 (Dense)              multiple                  304       
_________________________________________________________________
hidden2 (Dense)              multiple                  136       
_________________________________________________________________
dropout (Dropout)            multiple                  0         
_________________________________________________________________
output (Dense)               multiple                  81        
=================================================================
Total params: 527
Trainable params: 527
Non-trainable params: 0
_________________________________________________________________</code></pre>
<p>On entraine le modele binaire en choisissant un taux d’apprentissage de 0.01 et en ajoutant de la regularisation par “early stopping” basee sur l’AUC : on stoppe l’entrainement lorsque la metrique sur l’echantillon de validation cesse de s’ameliorer sur une succession de 10 epoques virtuelles.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>train_ds_bin <span class="op">=</span> df_to_dataset(train, batch_size <span class="op">=</span> TRAIN_BATCH_SIZE, dico <span class="op">=</span> dico_income_bin)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>val_ds_bin <span class="op">=</span> df_to_dataset(val, shuffle <span class="op">=</span> <span class="va">False</span>, batch_size <span class="op">=</span> TRAIN_BATCH_SIZE, </span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>dico <span class="op">=</span> dico_income_bin, nb_repet <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>test_ds_bin <span class="op">=</span> df_to_dataset(test, shuffle <span class="op">=</span> <span class="va">False</span>, batch_size <span class="op">=</span> TRAIN_BATCH_SIZE, </span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>dico <span class="op">=</span> dico_income_bin, nb_repet <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>modele_binaire.<span class="bu">compile</span>(optimizer <span class="op">=</span> tf.keras.optimizers.Adam(learning_rate <span class="op">=</span> <span class="fl">0.01</span>),</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>              loss <span class="op">=</span> <span class="st">&#39;binary_crossentropy&#39;</span>,</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>              metrics <span class="op">=</span> metriques_bin)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>              </span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>early_stopping <span class="op">=</span> tf.keras.callbacks.EarlyStopping(</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    monitor <span class="op">=</span> <span class="st">&#39;val_auc&#39;</span>, </span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    verbose <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    patience <span class="op">=</span> <span class="dv">10</span>,</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    mode <span class="op">=</span> <span class="st">&#39;max&#39;</span>,</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    restore_best_weights <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>history_binaire <span class="op">=</span> modele_binaire.fit(x <span class="op">=</span> train_ds_bin,</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>                    steps_per_epoch <span class="op">=</span> steps_per_epoch,</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>                    epochs <span class="op">=</span> NUM_CHECKPOINTS,</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>                    validation_data <span class="op">=</span> val_ds_bin,</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>                    callbacks <span class="op">=</span> [early_stopping],</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>                    verbose <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>                   )</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>                   </span></code></pre></div>
<pre><code>Epoch 1/20
156/156 - 4s - loss: 0.5909 - auc: 0.7513 - accuracy: 0.7044 - precision: 0.7224 - recall: 0.6697 - tp: 3363.0000 - fp: 1292.0000 - tn: 3670.0000 - fn: 1659.0000 - val_loss: 0.5512 - val_auc: 0.7855 - val_accuracy: 0.7389 - val_precision: 0.7489 - val_recall: 0.7217 - val_tp: 498.0000 - val_fp: 167.0000 - val_tn: 518.0000 - val_fn: 192.0000
Epoch 2/20
156/156 - 0s - loss: 0.5721 - auc: 0.7678 - accuracy: 0.7145 - precision: 0.7378 - recall: 0.6718 - tp: 3377.0000 - fp: 1200.0000 - tn: 3757.0000 - fn: 1650.0000 - val_loss: 0.5500 - val_auc: 0.7876 - val_accuracy: 0.7375 - val_precision: 0.7615 - val_recall: 0.6942 - val_tp: 479.0000 - val_fp: 150.0000 - val_tn: 535.0000 - val_fn: 211.0000
Epoch 3/20
156/156 - 0s - loss: 0.5671 - auc: 0.7726 - accuracy: 0.7236 - precision: 0.7499 - recall: 0.6778 - tp: 3412.0000 - fp: 1138.0000 - tn: 3812.0000 - fn: 1622.0000 - val_loss: 0.5536 - val_auc: 0.7839 - val_accuracy: 0.7367 - val_precision: 0.7377 - val_recall: 0.7377 - val_tp: 509.0000 - val_fp: 181.0000 - val_tn: 504.0000 - val_fn: 181.0000
Epoch 4/20
156/156 - 0s - loss: 0.5659 - auc: 0.7749 - accuracy: 0.7216 - precision: 0.7476 - recall: 0.6757 - tp: 3401.0000 - fp: 1148.0000 - tn: 3803.0000 - fn: 1632.0000 - val_loss: 0.5490 - val_auc: 0.7884 - val_accuracy: 0.7367 - val_precision: 0.7611 - val_recall: 0.6928 - val_tp: 478.0000 - val_fp: 150.0000 - val_tn: 535.0000 - val_fn: 212.0000
Epoch 5/20
156/156 - 0s - loss: 0.5639 - auc: 0.7759 - accuracy: 0.7267 - precision: 0.7541 - recall: 0.6791 - tp: 3417.0000 - fp: 1114.0000 - tn: 3838.0000 - fn: 1615.0000 - val_loss: 0.5476 - val_auc: 0.7873 - val_accuracy: 0.7389 - val_precision: 0.7566 - val_recall: 0.7072 - val_tp: 488.0000 - val_fp: 157.0000 - val_tn: 528.0000 - val_fn: 202.0000
Epoch 6/20
156/156 - 0s - loss: 0.5641 - auc: 0.7772 - accuracy: 0.7260 - precision: 0.7494 - recall: 0.6798 - tp: 3397.0000 - fp: 1136.0000 - tn: 3851.0000 - fn: 1600.0000 - val_loss: 0.5519 - val_auc: 0.7883 - val_accuracy: 0.7324 - val_precision: 0.7938 - val_recall: 0.6304 - val_tp: 435.0000 - val_fp: 113.0000 - val_tn: 572.0000 - val_fn: 255.0000
Epoch 7/20
156/156 - 1s - loss: 0.5618 - auc: 0.7788 - accuracy: 0.7289 - precision: 0.7568 - recall: 0.6890 - tp: 3503.0000 - fp: 1126.0000 - tn: 3774.0000 - fn: 1581.0000 - val_loss: 0.5476 - val_auc: 0.7882 - val_accuracy: 0.7375 - val_precision: 0.7599 - val_recall: 0.6971 - val_tp: 481.0000 - val_fp: 152.0000 - val_tn: 533.0000 - val_fn: 209.0000
Epoch 8/20
156/156 - 1s - loss: 0.5644 - auc: 0.7743 - accuracy: 0.7229 - precision: 0.7448 - recall: 0.6833 - tp: 3433.0000 - fp: 1176.0000 - tn: 3784.0000 - fn: 1591.0000 - val_loss: 0.5489 - val_auc: 0.7865 - val_accuracy: 0.7367 - val_precision: 0.7485 - val_recall: 0.7159 - val_tp: 494.0000 - val_fp: 166.0000 - val_tn: 519.0000 - val_fn: 196.0000
Epoch 9/20
156/156 - 1s - loss: 0.5624 - auc: 0.7773 - accuracy: 0.7282 - precision: 0.7482 - recall: 0.6956 - tp: 3506.0000 - fp: 1180.0000 - tn: 3764.0000 - fn: 1534.0000 - val_loss: 0.5466 - val_auc: 0.7885 - val_accuracy: 0.7389 - val_precision: 0.7534 - val_recall: 0.7130 - val_tp: 492.0000 - val_fp: 161.0000 - val_tn: 524.0000 - val_fn: 198.0000
Epoch 10/20
156/156 - 1s - loss: 0.5659 - auc: 0.7727 - accuracy: 0.7250 - precision: 0.7466 - recall: 0.6836 - tp: 3423.0000 - fp: 1162.0000 - tn: 3815.0000 - fn: 1584.0000 - val_loss: 0.5489 - val_auc: 0.7884 - val_accuracy: 0.7338 - val_precision: 0.7903 - val_recall: 0.6391 - val_tp: 441.0000 - val_fp: 117.0000 - val_tn: 568.0000 - val_fn: 249.0000
Epoch 11/20
156/156 - 0s - loss: 0.5641 - auc: 0.7763 - accuracy: 0.7253 - precision: 0.7516 - recall: 0.6850 - tp: 3470.0000 - fp: 1147.0000 - tn: 3771.0000 - fn: 1596.0000 - val_loss: 0.5586 - val_auc: 0.7830 - val_accuracy: 0.7345 - val_precision: 0.7379 - val_recall: 0.7304 - val_tp: 504.0000 - val_fp: 179.0000 - val_tn: 506.0000 - val_fn: 186.0000
Epoch 12/20
156/156 - 0s - loss: 0.5625 - auc: 0.7772 - accuracy: 0.7256 - precision: 0.7420 - recall: 0.6946 - tp: 3480.0000 - fp: 1210.0000 - tn: 3764.0000 - fn: 1530.0000 - val_loss: 0.5526 - val_auc: 0.7880 - val_accuracy: 0.7360 - val_precision: 0.7401 - val_recall: 0.7304 - val_tp: 504.0000 - val_fp: 177.0000 - val_tn: 508.0000 - val_fn: 186.0000
Epoch 13/20
156/156 - 1s - loss: 0.5645 - auc: 0.7751 - accuracy: 0.7237 - precision: 0.7462 - recall: 0.6873 - tp: 3470.0000 - fp: 1180.0000 - tn: 3755.0000 - fn: 1579.0000 - val_loss: 0.5474 - val_auc: 0.7891 - val_accuracy: 0.7367 - val_precision: 0.7419 - val_recall: 0.7290 - val_tp: 503.0000 - val_fp: 175.0000 - val_tn: 510.0000 - val_fn: 187.0000
Epoch 14/20
156/156 - 1s - loss: 0.5587 - auc: 0.7811 - accuracy: 0.7263 - precision: 0.7452 - recall: 0.6925 - tp: 3477.0000 - fp: 1189.0000 - tn: 3774.0000 - fn: 1544.0000 - val_loss: 0.5478 - val_auc: 0.7908 - val_accuracy: 0.7360 - val_precision: 0.7575 - val_recall: 0.6971 - val_tp: 481.0000 - val_fp: 154.0000 - val_tn: 531.0000 - val_fn: 209.0000
Epoch 15/20
156/156 - 0s - loss: 0.5625 - auc: 0.7778 - accuracy: 0.7269 - precision: 0.7448 - recall: 0.6954 - tp: 3493.0000 - fp: 1197.0000 - tn: 3764.0000 - fn: 1530.0000 - val_loss: 0.5485 - val_auc: 0.7903 - val_accuracy: 0.7404 - val_precision: 0.7542 - val_recall: 0.7159 - val_tp: 494.0000 - val_fp: 161.0000 - val_tn: 524.0000 - val_fn: 196.0000
Epoch 16/20
156/156 - 0s - loss: 0.5601 - auc: 0.7812 - accuracy: 0.7284 - precision: 0.7511 - recall: 0.6906 - tp: 3480.0000 - fp: 1153.0000 - tn: 3792.0000 - fn: 1559.0000 - val_loss: 0.5488 - val_auc: 0.7889 - val_accuracy: 0.7382 - val_precision: 0.7636 - val_recall: 0.6928 - val_tp: 478.0000 - val_fp: 148.0000 - val_tn: 537.0000 - val_fn: 212.0000
Epoch 17/20
156/156 - 0s - loss: 0.5601 - auc: 0.7801 - accuracy: 0.7277 - precision: 0.7461 - recall: 0.6964 - tp: 3502.0000 - fp: 1192.0000 - tn: 3763.0000 - fn: 1527.0000 - val_loss: 0.5513 - val_auc: 0.7866 - val_accuracy: 0.7396 - val_precision: 0.7538 - val_recall: 0.7145 - val_tp: 493.0000 - val_fp: 161.0000 - val_tn: 524.0000 - val_fn: 197.0000
Epoch 18/20
156/156 - 0s - loss: 0.5626 - auc: 0.7787 - accuracy: 0.7268 - precision: 0.7432 - recall: 0.6957 - tp: 3484.0000 - fp: 1204.0000 - tn: 3772.0000 - fn: 1524.0000 - val_loss: 0.5499 - val_auc: 0.7872 - val_accuracy: 0.7389 - val_precision: 0.7527 - val_recall: 0.7145 - val_tp: 493.0000 - val_fp: 162.0000 - val_tn: 523.0000 - val_fn: 197.0000
Epoch 19/20
156/156 - 0s - loss: 0.5565 - auc: 0.7845 - accuracy: 0.7291 - precision: 0.7494 - recall: 0.6988 - tp: 3534.0000 - fp: 1182.0000 - tn: 3745.0000 - fn: 1523.0000 - val_loss: 0.5491 - val_auc: 0.7900 - val_accuracy: 0.7353 - val_precision: 0.7500 - val_recall: 0.7087 - val_tp: 489.0000 - val_fp: 163.0000 - val_tn: 522.0000 - val_fn: 201.0000
Epoch 20/20
156/156 - 0s - loss: 0.5578 - auc: 0.7819 - accuracy: 0.7282 - precision: 0.7462 - recall: 0.7041 - tp: 3569.0000 - fp: 1214.0000 - tn: 3701.0000 - fn: 1500.0000 - val_loss: 0.5520 - val_auc: 0.7877 - val_accuracy: 0.7287 - val_precision: 0.7145 - val_recall: 0.7652 - val_tp: 528.0000 - val_fp: 211.0000 - val_tn: 474.0000 - val_fn: 162.0000

WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a &lt;class &#39;dict&#39;&gt; input: {&#39;number_in_household&#39;: &lt;tf.Tensor &#39;ExpandDims_2:0&#39; shape=(None, 1) dtype=int32&gt;, &#39;marital_status&#39;: &lt;tf.Tensor &#39;ExpandDims_1:0&#39; shape=(None, 1) dtype=string&gt;, &#39;householder_status&#39;: &lt;tf.Tensor &#39;ExpandDims:0&#39; shape=(None, 1) dtype=string&gt;}
Consider rewriting this model with the Functional API.
WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a &lt;class &#39;dict&#39;&gt; input: {&#39;number_in_household&#39;: &lt;tf.Tensor &#39;ExpandDims_2:0&#39; shape=(None, 1) dtype=int32&gt;, &#39;marital_status&#39;: &lt;tf.Tensor &#39;ExpandDims_1:0&#39; shape=(None, 1) dtype=string&gt;, &#39;householder_status&#39;: &lt;tf.Tensor &#39;ExpandDims:0&#39; shape=(None, 1) dtype=string&gt;}
Consider rewriting this model with the Functional API.
WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a &lt;class &#39;dict&#39;&gt; input: {&#39;number_in_household&#39;: &lt;tf.Tensor &#39;ExpandDims_2:0&#39; shape=(None, 1) dtype=int32&gt;, &#39;marital_status&#39;: &lt;tf.Tensor &#39;ExpandDims_1:0&#39; shape=(None, 1) dtype=string&gt;, &#39;householder_status&#39;: &lt;tf.Tensor &#39;ExpandDims:0&#39; shape=(None, 1) dtype=string&gt;}
Consider rewriting this model with the Functional API.</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>modele_binaire.summary()</span></code></pre></div>
<pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_features_1 (DenseFeatu multiple                  6         
_________________________________________________________________
hidden1 (Dense)              multiple                  304       
_________________________________________________________________
hidden2 (Dense)              multiple                  136       
_________________________________________________________________
dropout_1 (Dropout)          multiple                  0         
_________________________________________________________________
output (Dense)               multiple                  9         
=================================================================
Total params: 455
Trainable params: 455
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
<div id="performance-des-modeles" class="section level3">
<h3>Performance des modeles</h3>
<p>Fonction de perte et metriques sur l’echantillon de test.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">&quot;KPI&quot;</span>: modele_softmax.metrics_names, </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;valeurs&quot;</span>: modele_softmax.evaluate(test_ds, verbose <span class="op">=</span> <span class="dv">0</span>)})</span></code></pre></div>
<pre><code>        KPI   valeurs
0      loss  1.875248
1  accuracy  0.300872</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">&quot;KPI&quot;</span>: modele_binaire.metrics_names, </span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;valeurs&quot;</span>: modele_binaire.evaluate(test_ds_bin, verbose <span class="op">=</span> <span class="dv">0</span>)})</span></code></pre></div>
<pre><code>         KPI     valeurs
0       loss    0.537471
1        auc    0.803664
2   accuracy    0.741279
3  precision    0.729378
4     recall    0.748886
5         tp  504.000000
6         fp  187.000000
7         tn  516.000000
8         fn  169.000000</code></pre>
<p>Graphiques des performances du modele multi-classes selon l’epoque virtuelle.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>dtf <span class="op">=</span> pd.DataFrame(history_sofmax.history)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>colonnes <span class="op">=</span> dtf.columns</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>dtf <span class="op">=</span> dtf.reset_index().rename(columns <span class="op">=</span> {<span class="st">&#39;index&#39;</span>: <span class="st">&#39;virtual_epochs&#39;</span>})</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>dtf_tr <span class="op">=</span> pd.melt(dtf, id_vars <span class="op">=</span> [<span class="st">&#39;virtual_epochs&#39;</span>], value_vars <span class="op">=</span> colonnes, </span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>var_name <span class="op">=</span><span class="st">&#39;noms_col&#39;</span>, value_name<span class="op">=</span><span class="st">&#39;valeurs_col&#39;</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>dtf_tr[<span class="st">&quot;metrique&quot;</span>] <span class="op">=</span> dtf_tr.noms_col.<span class="bu">str</span>.replace(<span class="st">&quot;^val_&quot;</span>, <span class="st">&quot;&quot;</span>, regex <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>dtf_tr[<span class="st">&quot;echantillon&quot;</span>] <span class="op">=</span> <span class="st">&quot;val&quot;</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>dtf_tr.loc[dtf_tr.noms_col <span class="op">==</span> dtf_tr.metrique, <span class="st">&quot;echantillon&quot;</span>] <span class="op">=</span> <span class="st">&quot;train&quot;</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> sns.FacetGrid(dtf_tr, col<span class="op">=</span><span class="st">&quot;metrique&quot;</span>, hue <span class="op">=</span> <span class="st">&quot;echantillon&quot;</span>, sharey <span class="op">=</span> <span class="va">False</span>, col_wrap <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> g.map_dataframe(sns.lineplot, x <span class="op">=</span> <span class="st">&quot;virtual_epochs&quot;</span>,y <span class="op">=</span> <span class="st">&quot;valeurs_col&quot;</span>).add_legend()</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="hebdor_dnn_Tensorflow_files/figure-html/unnamed-chunk-15-1.png" width="600px" height="300px" style="display: block; margin: auto;" /></p>
<p>Graphiques des performances du modele binaire selon l’epoque virtuelle.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>dtf <span class="op">=</span> pd.DataFrame(history_binaire.history)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>colonnes <span class="op">=</span> dtf.columns</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>dtf <span class="op">=</span> dtf.reset_index().rename(columns <span class="op">=</span> {<span class="st">&#39;index&#39;</span>: <span class="st">&#39;virtual_epochs&#39;</span>})</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>dtf_tr <span class="op">=</span> pd.melt(dtf, id_vars <span class="op">=</span> [<span class="st">&#39;virtual_epochs&#39;</span>], value_vars <span class="op">=</span> colonnes, </span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>var_name <span class="op">=</span><span class="st">&#39;noms_col&#39;</span>, value_name<span class="op">=</span><span class="st">&#39;valeurs_col&#39;</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>dtf_tr[<span class="st">&quot;metrique&quot;</span>] <span class="op">=</span> dtf_tr.noms_col.<span class="bu">str</span>.replace(<span class="st">&quot;^val_&quot;</span>, <span class="st">&quot;&quot;</span>, regex <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>dtf_tr[<span class="st">&quot;echantillon&quot;</span>] <span class="op">=</span> <span class="st">&quot;val&quot;</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>dtf_tr.loc[dtf_tr.noms_col <span class="op">==</span> dtf_tr.metrique, <span class="st">&quot;echantillon&quot;</span>] <span class="op">=</span> <span class="st">&quot;train&quot;</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> sns.FacetGrid(dtf_tr, col<span class="op">=</span><span class="st">&quot;metrique&quot;</span>, hue <span class="op">=</span> <span class="st">&quot;echantillon&quot;</span>, sharey <span class="op">=</span> <span class="va">False</span>, col_wrap <span class="op">=</span> <span class="dv">3</span>)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> g.map_dataframe(sns.lineplot, x <span class="op">=</span> <span class="st">&quot;virtual_epochs&quot;</span>,y <span class="op">=</span> <span class="st">&quot;valeurs_col&quot;</span>).add_legend()</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="hebdor_dnn_Tensorflow_files/figure-html/unnamed-chunk-16-1.png" width="700px" height="700px" style="display: block; margin: auto;" /></p>
<p>Les scores du modele multi-classes.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># on recupere un array numpy</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> modele_softmax.predict(test_ds)</span></code></pre></div>
<pre><code>WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a &lt;class &#39;dict&#39;&gt; input: {&#39;number_in_household&#39;: &lt;tf.Tensor &#39;ExpandDims_2:0&#39; shape=(None, 1) dtype=int32&gt;, &#39;marital_status&#39;: &lt;tf.Tensor &#39;ExpandDims_1:0&#39; shape=(None, 1) dtype=string&gt;, &#39;householder_status&#39;: &lt;tf.Tensor &#39;ExpandDims:0&#39; shape=(None, 1) dtype=string&gt;}
Consider rewriting this model with the Functional API.</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>predictions.<span class="bu">round</span>(<span class="dv">2</span>)[:<span class="dv">5</span>]</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="co"># controle : somme des probas = 1</span></span></code></pre></div>
<pre><code>array([[0.01, 0.02, 0.03, 0.05, 0.05, 0.15, 0.17, 0.3 , 0.22],
       [0.12, 0.14, 0.11, 0.15, 0.13, 0.15, 0.11, 0.07, 0.02],
       [0.01, 0.02, 0.03, 0.05, 0.05, 0.15, 0.17, 0.3 , 0.22],
       [0.18, 0.14, 0.11, 0.14, 0.11, 0.13, 0.09, 0.07, 0.03],
       [0.06, 0.09, 0.08, 0.13, 0.12, 0.18, 0.16, 0.14, 0.06]],
      dtype=float32)</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>tf.reduce_sum(tf.nn.softmax(predictions[:<span class="dv">5</span>]), axis <span class="op">=</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>&lt;tf.Tensor: shape=(5,), dtype=float32, numpy=
array([1.       , 1.0000001, 1.       , 1.       , 1.       ],
      dtype=float32)&gt;</code></pre>
<p>La matrice de confusion du modele multi-classes sur l’echantillon test.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># les previsions, pour chaque exemple on retient la colonne de score maximal </span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>prev <span class="op">=</span> pd.DataFrame(predictions)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>prev <span class="op">=</span> <span class="bu">list</span>(prev.idxmax(axis <span class="op">=</span> <span class="st">&#39;columns&#39;</span>))</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co"># le reel (les labels sont la 2eme composante du dataset de test)</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>reel <span class="op">=</span> pd.concat([pd.DataFrame(dts[<span class="dv">1</span>], columns <span class="op">=</span> [<span class="st">&quot;x&quot;</span>]) <span class="cf">for</span> dts <span class="kw">in</span> <span class="bu">iter</span>(test_ds)])</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">&#39;Y&#39;</span>: <span class="bu">list</span>(reel.x), <span class="st">&#39;Ypred&#39;</span>: prev}).groupby([<span class="st">&quot;Y&quot;</span>, <span class="st">&quot;Ypred&quot;</span>]).size(</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>).reset_index(name <span class="op">=</span> <span class="st">&quot;nb&quot;</span>).pivot(index<span class="op">=</span><span class="st">&quot;Y&quot;</span>, columns<span class="op">=</span><span class="st">&quot;Ypred&quot;</span>)</span></code></pre></div>
<pre><code>          nb                  
Ypred      0    1     5      7
Y                             
0      225.0  3.0  19.0    8.0
1       67.0  1.0  26.0    9.0
2       66.0  2.0  38.0   13.0
3       61.0  3.0  34.0   29.0
4       44.0  1.0  33.0   21.0
5       42.0  1.0  58.0   53.0
6       35.0  NaN  40.0   68.0
7       40.0  NaN  43.0  130.0
8       30.0  NaN  17.0  116.0</code></pre>
<p>La matrice de confusion du modele binaire sur l’echantillon test.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>prev <span class="op">=</span> modele_binaire.predict(test_ds_bin).squeeze()</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># on choisit un seuil : 0.5 par defaut dans les metriques precedentes (vrais positifs, ...)</span></span></code></pre></div>
<pre><code>WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a &lt;class &#39;dict&#39;&gt; input: {&#39;number_in_household&#39;: &lt;tf.Tensor &#39;ExpandDims_2:0&#39; shape=(None, 1) dtype=int32&gt;, &#39;marital_status&#39;: &lt;tf.Tensor &#39;ExpandDims_1:0&#39; shape=(None, 1) dtype=string&gt;, &#39;householder_status&#39;: &lt;tf.Tensor &#39;ExpandDims:0&#39; shape=(None, 1) dtype=string&gt;}
Consider rewriting this model with the Functional API.</code></pre>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>seuil <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>prev <span class="op">=</span> (prev <span class="op">&gt;=</span> seuil).astype(<span class="bu">int</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="co"># le reel </span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>reel <span class="op">=</span> pd.concat([pd.DataFrame(dts[<span class="dv">1</span>], columns <span class="op">=</span> [<span class="st">&quot;x&quot;</span>]) <span class="cf">for</span> dts <span class="kw">in</span> <span class="bu">iter</span>(test_ds_bin)])</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">&#39;Y&#39;</span>: <span class="bu">list</span>(reel.x), <span class="st">&#39;Ypred&#39;</span>: prev}).groupby([<span class="st">&quot;Y&quot;</span>, <span class="st">&quot;Ypred&quot;</span>]).size(</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>  ).reset_index(name <span class="op">=</span> <span class="st">&quot;nb&quot;</span>).pivot(index<span class="op">=</span><span class="st">&quot;Y&quot;</span>, columns<span class="op">=</span><span class="st">&quot;Ypred&quot;</span>)</span></code></pre></div>
<pre><code>        nb     
Ypred    0    1
Y              
0      516  187
1      169  504</code></pre>
</div>
</div>
<div id="modelisation-multi-classes-avec-tf.estimator" class="section level1">
<h1>Modelisation multi-classes avec <em>tf.estimator</em></h1>
<p>On n’utilise pas l’API Keras cette fois mais <em>tf.estimator</em>, ce qui faciliterait l’enchainement de pretraitements, de modelisation et de deploiement sur la Google Cloud Platform.</p>
<p>Fonction de pre-traitement basee sur <em>pandas_input_fn</em>.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_input_fn(dataframe, shuffle<span class="op">=</span> <span class="va">True</span>, batch_size <span class="op">=</span> TRAIN_BATCH_SIZE, </span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>dico <span class="op">=</span> dico_income, num_epochs<span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>  dataframe <span class="op">=</span> dataframe.copy()</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>  labels <span class="op">=</span> dataframe.pop(<span class="st">&#39;income&#39;</span>).<span class="bu">map</span>(dico)</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> tf.compat.v1.estimator.inputs.pandas_input_fn(</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> dataframe,</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> labels,</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> batch_size,</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>    num_epochs <span class="op">=</span> num_epochs,</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>    shuffle <span class="op">=</span> shuffle,</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    queue_capacity <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>    num_threads <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p>On recupere les champs calcules plus haut.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_feature_cols():</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> feature_columns.values()</span></code></pre></div>
<p>On definit deux classifieurs</p>
<ul>
<li>le <em>LinearClassifier</em> qui est simplement une regression logistique</li>
<li>le <em>DNNClassifier</em> qui est un reseau a plusieus couches cachees</li>
</ul>
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>output_dir <span class="op">=</span> <span class="st">&#39;./tmp_tensorflow/trained_model&#39;</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="co"># classifieur lineaire</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>myopt <span class="op">=</span> tf.compat.v1.train.FtrlOptimizer(learning_rate <span class="op">=</span> <span class="fl">0.2</span>, l1_regularization_strength <span class="op">=</span> <span class="fl">0.1</span>)</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>estimator_lin <span class="op">=</span> tf.compat.v1.estimator.LinearClassifier(model_dir <span class="op">=</span> output_dir, </span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>n_classes<span class="op">=</span> <span class="bu">len</span>(dico_income) ,feature_columns <span class="op">=</span> create_feature_cols(), optimizer <span class="op">=</span> myopt)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="co"># classifieur DNN a 2 couches cachees</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>estimator_dnn <span class="op">=</span> tf.compat.v1.estimator.DNNClassifier(hidden_units <span class="op">=</span> [<span class="dv">16</span>, <span class="dv">8</span>], </span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>model_dir <span class="op">=</span> output_dir, n_classes<span class="op">=</span> <span class="bu">len</span>(dico_income) ,feature_columns <span class="op">=</span> create_feature_cols())</span></code></pre></div>
<p>On precise quelques parametres pour l’apprentissage et le test.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>num_train_steps <span class="op">=</span> steps_per_epoch <span class="op">*</span> NUM_CHECKPOINTS</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co"># pour le jeu d&#39;entrainement</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>train_spec <span class="op">=</span> tf.estimator.TrainSpec(input_fn <span class="op">=</span> make_input_fn(train, shuffle<span class="op">=</span><span class="va">True</span>, num_epochs<span class="op">=</span><span class="va">None</span>), </span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>                                    max_steps <span class="op">=</span> num_train_steps)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="co"># pour le jeu de validation, on evalue apres start_delay_secs secondes</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="co"># et on evalue toutes les throttle_secs secondes</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>eval_spec <span class="op">=</span> tf.estimator.EvalSpec(input_fn <span class="op">=</span> make_input_fn(val, shuffle <span class="op">=</span> <span class="va">False</span>, num_epochs <span class="op">=</span> <span class="dv">1</span>), </span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>                                  steps <span class="op">=</span> <span class="va">None</span>, </span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>                                  start_delay_secs <span class="op">=</span> <span class="dv">2</span>,</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>                                  throttle_secs <span class="op">=</span> <span class="dv">4</span>) </span></code></pre></div>
<p>On entraine et on evalue les deux modeles.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># on vide le repertoire et on le supprime car il sert aussi de checkpoint </span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="co"># et son contenu peut etre recharge pour poursuivre l&#39;entrainement</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>shutil.rmtree(output_dir, ignore_errors <span class="op">=</span> <span class="va">True</span>) </span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>tf.estimator.train_and_evaluate(estimator_lin, train_spec, eval_spec)</span></code></pre></div>
<pre><code>({&#39;accuracy&#39;: 0.26618183, &#39;average_loss&#39;: 1.9385524, &#39;loss&#39;: 121.15952, &#39;global_step&#39;: 3120}, [])

WARNING:tensorflow:From C:\Users\SEBAST~1\ANACON~1\envs\tf_env\lib\site-packages\tensorflow\python\training\training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
WARNING:tensorflow:From C:\Users\SEBAST~1\ANACON~1\envs\tf_env\lib\site-packages\tensorflow_estimator\python\estimator\inputs\queues\feeding_queue_runner.py:60: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From C:\Users\SEBAST~1\ANACON~1\envs\tf_env\lib\site-packages\tensorflow_estimator\python\estimator\inputs\queues\feeding_functions.py:491: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From C:\Users\SEBAST~1\ANACON~1\envs\tf_env\lib\site-packages\tensorflow_estimator\python\estimator\canned\linear.py:1471: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.add_weight` method instead.
WARNING:tensorflow:From C:\Users\SEBAST~1\ANACON~1\envs\tf_env\lib\site-packages\tensorflow\python\training\monitored_session.py:906: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 2380 vs previous value: 2380. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.</code></pre>
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>shutil.rmtree(output_dir, ignore_errors <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>tf.estimator.train_and_evaluate(estimator_dnn, train_spec, eval_spec)</span></code></pre></div>
<pre><code>({&#39;accuracy&#39;: 0.28218183, &#39;average_loss&#39;: 1.91672, &#39;loss&#39;: 119.795, &#39;global_step&#39;: 3120}, [])

WARNING:tensorflow:AutoGraph could not transform &lt;bound method _DNNModel.call of &lt;tensorflow_estimator.python.estimator.canned.dnn._DNNModel object at 0x000000000EC6C940&gt;&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module &#39;gast&#39; has no attribute &#39;Index&#39;
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:From C:\Users\SEBAST~1\ANACON~1\envs\tf_env\lib\site-packages\tensorflow\python\training\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor</code></pre>
<p><a href="#intro"><strong>retour au debut du document</strong></a></p>
</div>
</section>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
